{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Using GPT-5 - OpenAI API Guide\n",
    "\n",
    "## Introduction to OpenAI's Most Intelligent Model\n",
    "\n",
    "GPT-5 is OpenAI's most advanced reasoning model, specifically trained for:\n",
    "- **Code generation**, bug fixing, and refactoring\n",
    "- **Instruction following** with high accuracy\n",
    "- **Long context** handling and **tool calling** for agentic tasks\n",
    "\n",
    "This notebook demonstrates GPT-5's key features with practical code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-cell",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the OpenAI Python client if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI client\n",
    "# Run this if you're running this notebook on google colab\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")  # Set your API key as environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-overview",
   "metadata": {},
   "source": [
    "## GPT-5 Model Variants\n",
    "\n",
    "| Model | Best For | Trade-offs |\n",
    "|-------|----------|------------|\n",
    "| **`gpt-5`** | Complex reasoning, broad knowledge, code-heavy tasks | Highest capability, higher latency |\n",
    "| **`gpt-5-mini`** | Cost-optimized reasoning and chat | Balanced speed, cost, and capability |\n",
    "| **`gpt-5-nano`** | High-throughput, simple tasks | Fastest, most cost-effective |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123b33e",
   "metadata": {},
   "source": [
    "System card uses different names than the API, OpenAI provides this mapping table:\n",
    "\n",
    "<img src=\"../assets/map-api-system-names.png\" width=40%>\n",
    "\n",
    "source: https://platform.openai.com/docs/guides/latest-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart-title",
   "metadata": {},
   "source": [
    "## Quickstart: Fast Responses with Low Reasoning\n",
    "\n",
    "For faster, lower-latency responses similar to GPT-4.1, use **low reasoning effort** and **low verbosity**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quickstart-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Silent loops hum on—\n",
      "logic weaves midnight patterns,\n",
      "bugs bloom, dawn reveals.\n",
      "\n",
      "Reasoning tokens used: N/A\n"
     ]
    }
   ],
   "source": [
    "# Fast response with minimal reasoning\n",
    "result = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a haiku about code.\",\n",
    "    reasoning={\"effort\": \"low\"},\n",
    ")\n",
    "\n",
    "print(\"Output:\", result.output_text)\n",
    "print(\"\\nReasoning tokens used:\", len(result.reasoning_text.split()) if hasattr(result, 'reasoning_text') else 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasoning-levels",
   "metadata": {},
   "source": [
    "## Reasoning Effort Control\n",
    "\n",
    "GPT-5 supports four reasoning levels: `minimal` (new one!) `low`, `medium`, `high`\n",
    "\n",
    "- **`minimal`**: Fastest time-to-first-token, best for coding & instruction following\n",
    "- **`low`**: Quick responses with light reasoning\n",
    "- **`medium`**: Default, balanced reasoning (similar to o3)\n",
    "- **`high`**: Most thorough reasoning for complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minimal-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal Reasoning Output:\n",
      "Here’s a concise and efficient Python function to check if an integer is prime:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def is_prime(n: int) -> bool:\n",
      "    # Handle small and negative numbers\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True  # 2 and 3 are prime\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "\n",
      "    # Check divisibility by numbers of the form 6k ± 1 up to sqrt(n)\n",
      "    limit = int(math.isqrt(n))\n",
      "    i = 5\n",
      "    while i <= limit:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "```\n",
      "\n",
      "Notes:\n",
      "- Uses early exits for small cases and even/3-multiples.\n",
      "- Only tests potential factors up to sqrt(n).\n",
      "- Iterates using 6k ± 1 optimization for fewer checks.\n"
     ]
    }
   ],
   "source": [
    "# Minimal reasoning for fastest response\n",
    "response_minimal = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a Python function to check if a number is prime.\",\n",
    "    reasoning={\"effort\": \"minimal\"}\n",
    ")\n",
    "\n",
    "print(\"Minimal Reasoning Output:\")\n",
    "print(response_minimal.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "high-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Reasoning Output:\n",
      "Short answer: about 25–30 metric tons of gold. A reasonable estimate using the statue’s known copper skin gives about 25.5 metric tons.\n",
      "\n",
      "Assumptions and data\n",
      "- Coating only the statue (not the pedestal), exterior surface only\n",
      "- Copper skin thickness of the statue: 3/32 in = 0.09375 in = 0.00238125 m\n",
      "- Mass of the statue’s copper skin: 62,000 lb = 28,124 kg\n",
      "- Density of copper ρCu = 8,960 kg/m^3\n",
      "- Density of gold ρAu = 19,320 kg/m^3\n",
      "- Desired gold thickness tAu = 1 mm = 0.001 m\n",
      "\n",
      "Steps\n",
      "1) Infer the statue’s exterior surface area A from the copper skin:\n",
      "   A = mCu / (ρCu × tCu)\n",
      "     = 28,124 kg / (8,960 kg/m^3 × 0.00238125 m)\n",
      "     ≈ 28,124 / 21.336\n",
      "     ≈ 1,318 m^2\n",
      "\n",
      "2) Volume of gold needed for a 1 mm coat:\n",
      "   VAu = A × tAu = 1,318 m^2 × 0.001 m = 1.318 m^3\n",
      "\n",
      "3) Mass of gold:\n",
      "   mAu = ρAu × VAu = 19,320 kg/m^3 × 1.318 m^3 ≈ 25,458 kg\n",
      "\n",
      "Result\n",
      "- Gold required ≈ 25,500 kg ≈ 25.5 metric tons (≈ 28.1 short tons)\n",
      "\n",
      "Note\n",
      "- If you prefer to use a different surface-area estimate, the quick rule is:\n",
      "  mAu (kg) ≈ 19.32 × A (m^2) for a 1 mm coat.\n",
      "  For example, if A were 1,800 m^2, you’d need ≈ 34.8 metric tons.\n"
     ]
    }
   ],
   "source": [
    "# High reasoning for complex problems\n",
    "response_high = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"How much gold would it take to coat the Statue of Liberty in a 1mm layer? Show your calculations.\",\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "\n",
    "print(\"High Reasoning Output:\")\n",
    "print(response_high.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbosity-title",
   "metadata": {},
   "source": [
    "## Verbosity Control\n",
    "\n",
    "Control output length with `verbosity` parameter:\n",
    "- **`low`**: Concise answers, minimal code comments\n",
    "- **`medium`**: Balanced explanations (default)\n",
    "- **`high`**: Thorough explanations, detailed code documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verbosity-low",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise Output:\n",
      "SELECT\n",
      "  c.id AS customer_id,\n",
      "  c.name AS customer_name,\n",
      "  SUM(oi.quantity * oi.unit_price) AS total_purchase\n",
      "FROM customers c\n",
      "JOIN orders o        ON o.customer_id = c.id\n",
      "JOIN order_items oi  ON oi.order_id = o.id\n",
      "WHERE o.status = 'completed'\n",
      "GROUP BY c.id, c.name\n",
      "ORDER BY total_purchase DESC\n",
      "LIMIT 5;\n"
     ]
    }
   ],
   "source": [
    "# Low verbosity for concise responses\n",
    "response_concise = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Generate a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "print(\"Concise Output:\")\n",
    "print(response_concise.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verbosity-high",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Output:\n",
      "Async/await is JavaScript’s way to write asynchronous code that looks and reads like synchronous code, while still being non-blocking under the hood. It is built on top of Promises and the event loop.\n",
      "\n",
      "Core ideas\n",
      "- A Promise represents a value that will be available later (fulfilled) or fail (rejected).\n",
      "- An async function always returns a Promise.\n",
      "  - If you return a value, it becomes Promise.resolve(value).\n",
      "  - If you throw, it becomes Promise.reject(error).\n",
      "- await “pauses” the async function...\n"
     ]
    }
   ],
   "source": [
    "# High verbosity for detailed explanations\n",
    "response_detailed = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Explain how async/await works in JavaScript.\",\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "print(\"Detailed Output:\")\n",
    "print(response_detailed.output_text[:500] + \"...\")  # Truncated for display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731da33",
   "metadata": {},
   "source": [
    "- High verbosity: Use when you need the model to provide thorough explanations of documents or perform extensive code refactoring.\n",
    "\n",
    "- Low verbosity: Best for situations where you want concise answers or simple code generation, such as SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-tools-title",
   "metadata": {},
   "source": [
    "## Custom Tools: Freeform Text Inputs\n",
    "\n",
    "GPT-5 introduces **custom tools** that accept raw text instead of structured JSON. Perfect for:\n",
    "- Executing code snippets\n",
    "- SQL queries\n",
    "- Shell commands\n",
    "- Configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "custom-tool-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call Generated:\n",
      "Tool: code_exec\n",
      "Input: import math\n",
      "print(math.factorial(10))\n"
     ]
    }
   ],
   "source": [
    "# Define a custom tool for code execution\n",
    "response_with_tool = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Use the code_exec tool to calculate the factorial of 10.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"code_exec\",\n",
    "            \"description\": \"Executes arbitrary Python code that prints something and returns the result\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Tool Call Generated:\")\n",
    "for reasoning_output in response_with_tool.output:\n",
    "    if reasoning_output.type == \"custom_tool_call\":\n",
    "        print(f\"Tool: {reasoning_output.name}\")\n",
    "        print(f\"Input: {reasoning_output.input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810946c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3628800'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def code_exec(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes arbitrary Python code and returns the result as a string.\n",
    "    WARNING: This function is dangerous and should only be used in secure, sandboxed environments.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import io\n",
    "    import traceback\n",
    "\n",
    "    # Redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    result = \"\"\n",
    "    try:\n",
    "        # Try to compile as an expression first\n",
    "        try:\n",
    "            compiled = compile(code, \"<string>\", \"eval\")\n",
    "            output = eval(compiled, {}, {})\n",
    "            if output is not None:\n",
    "                print(output)\n",
    "        except SyntaxError:\n",
    "            # If not an expression, treat as statements\n",
    "            exec(code, {}, {})\n",
    "        result = sys.stdout.getvalue()\n",
    "    except Exception:\n",
    "        result = \"Error:\\n\" + traceback.format_exc()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return result.strip()\n",
    "\n",
    "code_exec(reasoning_output.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg-title",
   "metadata": {},
   "source": [
    "## Context-Free Grammar (CFG) Constraints\n",
    "\n",
    "Constrain custom tool outputs to specific syntax using Lark grammars (or regex grammrs). \n",
    "\n",
    "We'll see a detailed example in notebook 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allowed-tools-title",
   "metadata": {},
   "source": [
    "## Allowed Tools: Selective Tool Access\n",
    "\n",
    "Define a full toolkit but restrict which tools can be used in specific contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "allowed-tools-example",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_68d691511d5081978f965991433591860257326b2b51fa3d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{}', call_id='call_jmfCsbT4SMmwhJpplgvZojSl', name='get_weather', type='function_call', id='fc_68d69155d3148197a52b0a1e93bd5dfb0257326b2b51fa3d', status='completed'),\n",
       " ResponseFunctionToolCall(arguments='{}', call_id='call_PArxuLpuaC7kF6Bq01RPvkMr', name='search_docs', type='function_call', id='fc_68d69155e72081979fbeedce747ba4230257326b2b51fa3d', status='completed')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define multiple tools but restrict usage\n",
    "all_tools = [\n",
    "    {\"type\": \"function\", \"name\": \"get_weather\", \"description\": \"Get current weather\"},\n",
    "    {\"type\": \"function\", \"name\": \"search_docs\", \"description\": \"Search documentation\"},\n",
    "    {\"type\": \"function\", \"name\": \"run_tests\", \"description\": \"Execute test suite\"},\n",
    "    {\"type\": \"function\", \"name\": \"deploy_code\", \"description\": \"Deploy to production\"}\n",
    "]\n",
    "\n",
    "# Only allow safe operations\n",
    "response_restricted = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What's the weather like and can you search for React hooks documentation?\",\n",
    "    tools=all_tools,\n",
    "    tool_choice={\n",
    "        \"type\": \"allowed_tools\",\n",
    "        \"mode\": \"auto\",  # Model decides which to use\n",
    "        \"tools\": [\n",
    "            {\"type\": \"function\", \"name\": \"get_weather\"},\n",
    "            {\"type\": \"function\", \"name\": \"search_docs\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "response_restricted.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fc0c589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function', 'name': 'get_weather'},\n",
       " {'type': 'function', 'name': 'search_docs'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_restricted.tool_choice.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preambles-title",
   "metadata": {},
   "source": [
    "## Tool Preambles for Transparency\n",
    "\n",
    "Enable preambles to see GPT-5's reasoning before tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "preambles-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with preamble:\n",
      "I will use the search tool to fetch authoritative, up-to-date references and tutorials on Python decorators so I can provide accurate and concise information.\n"
     ]
    }
   ],
   "source": [
    "# Enable preambles for tool transparency\n",
    "response_preamble = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Before you call a tool, explain why you are calling it. Now search for information about Python decorators.\",\n",
    "    tools=[\n",
    "        {\"type\": \"function\", \"name\": \"search_docs\", \"description\": \"Search documentation\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response with preamble:\")\n",
    "print(response_preamble.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-title",
   "metadata": {},
   "source": [
    "## Migration Guide\n",
    "\n",
    "### From Other Models to GPT-5\n",
    "\n",
    "| From Model | Migrate To | Recommended Settings |\n",
    "|------------|------------|---------------------|\n",
    "| **o3** | `gpt-5` | `reasoning.effort: \"medium\"` or `\"high\"` |\n",
    "| **gpt-4.1** | `gpt-5` | `reasoning.effort: \"minimal\"` or `\"low\"` |\n",
    "| **o4-mini** | `gpt-5-mini` | Default settings with prompt tuning |\n",
    "| **gpt-4.1-nano** | `gpt-5-nano` | Default settings with prompt tuning |\n",
    "\n",
    "### ⚠️ Important: Unsupported Parameters\n",
    "\n",
    "GPT-5 does **NOT** support:\n",
    "- `temperature`\n",
    "- `top_p` \n",
    "- `logprobs`\n",
    "\n",
    "Use GPT-5-specific controls instead:\n",
    "- `reasoning: {effort: ...}`\n",
    "- `text: {verbosity: ...}`\n",
    "- `max_output_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responses-api-title",
   "metadata": {},
   "source": [
    "## Responses API vs Chat Completions\n",
    "\n",
    "### Key Advantage: Chain of Thought (CoT) Persistence\n",
    "\n",
    "The Responses API passes reasoning between turns, resulting in:\n",
    "- Improved intelligence\n",
    "- Fewer reasoning tokens generated\n",
    "- Higher cache hit rates\n",
    "- Lower latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "multi-turn-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: “Optimal” depends on what you need:\n",
      "\n",
      "- Fastest and simplest for function memoization: use functools.lru_cache (implemented in C).\n",
      "- General-purpose key/value LRU cache with O(1) get/put: use OrderedDi...\n",
      "\n",
      "Follow-up (with CoT context): Here’s a thread-safe LRU cache (O(1) get/put) built on OrderedDict with an internal RLock. It snapshots iteration results to avoid races during traversal.\n",
      "\n",
      "from collections import OrderedDict\n",
      "import t...\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with CoT persistence\n",
    "first_response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Let's solve a complex problem. What's the optimal way to implement a LRU cache in Python?\",\n",
    "    reasoning={\"effort\": \"medium\"}\n",
    ")\n",
    "\n",
    "print(\"First response:\", first_response.output_text[:200] + \"...\")\n",
    "\n",
    "# Continue conversation, passing previous response ID\n",
    "follow_up = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Now add thread-safety to that implementation.\",\n",
    "    previous_response_id=first_response.id  # Passes CoT from previous turn\n",
    ")\n",
    "\n",
    "print(\"\\nFollow-up (with CoT context):\", follow_up.output_text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-title",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Choose the Right Model\n",
    "- **`gpt-5`**: Complex reasoning, coding, multi-step tasks\n",
    "- **`gpt-5-mini`**: General chat, moderate complexity\n",
    "- **`gpt-5-nano`**: Simple tasks, high throughput\n",
    "\n",
    "### 2. Optimize for Your Use Case\n",
    "- **Speed Priority**: Use `minimal` reasoning + `low` verbosity\n",
    "- **Quality Priority**: Use `high` reasoning + `high` verbosity\n",
    "- **Balanced**: Use defaults (`medium` for both)\n",
    "\n",
    "### 3. Leverage New Features\n",
    "- **Custom Tools**: For code execution, SQL, configs\n",
    "- **Allowed Tools**: For safety and predictability\n",
    "- **Preambles**: For debugging and transparency\n",
    "\n",
    "### 4. Use Responses API for Multi-turn\n",
    "- Always pass `previous_response_id` for context\n",
    "- Reduces re-reasoning and improves coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-example-title",
   "metadata": {},
   "source": [
    "## Practical Example: Building a Code Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "code-assistant-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a version with error handling and logging:\n",
      "\n",
      "```python\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def divide(a, b):\n",
      "    \"\"\"\n",
      "    Divide a by b with logging and error handling.\n",
      "\n",
      "    - Logs inputs and result at DEBUG level.\n",
      "    - Logs and re-raises exceptions for invalid inputs or division by zero.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        logger.debug(\"divide called with a=%r, b=%r\", a, b)\n",
      "        result = a / b\n",
      "    except ZeroDivisionError:\n",
      "        logger.exception(\"Division by zero: a=%r, b=%r\", a, b)\n",
      "        raise\n",
      "    except TypeError:\n",
      "        logger.exception(\n",
      "            \"Unsupported operand types for division: a=%r (%s), b=%r (%s)\",\n",
      "            a, type(a).__name__, b, type(b).__name__\n",
      "        )\n",
      "        raise\n",
      "    except Exception:\n",
      "        logger.exception(\"Unexpected error during division: a=%r, b=%r\", a, b)\n",
      "        raise\n",
      "    else:\n",
      "        logger.debug(\"division result: %r\", result)\n",
      "        return result\n",
      "```\n",
      "\n",
      "Note:\n",
      "- In application code, configure logging once (avoid doing this in libraries/modules):\n",
      "  ```python\n",
      "  import logging\n",
      "  logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s:%(name)s:%(message)s\")\n",
      "  ```\n"
     ]
    }
   ],
   "source": [
    "def code_assistant(task, code_context=None, optimize_for=\"balanced\"):\n",
    "    \"\"\"\n",
    "    GPT-5 powered code assistant with configurable optimization.\n",
    "    \n",
    "    Args:\n",
    "        task: What you want the assistant to do\n",
    "        code_context: Existing code to work with\n",
    "        optimize_for: \"speed\", \"quality\", or \"balanced\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure based on optimization preference\n",
    "    settings = {\n",
    "        \"speed\": {\"reasoning\": {\"effort\": \"minimal\"}, \"text\": {\"verbosity\": \"low\"}},\n",
    "        \"quality\": {\"reasoning\": {\"effort\": \"high\"}, \"text\": {\"verbosity\": \"high\"}},\n",
    "        \"balanced\": {\"reasoning\": {\"effort\": \"medium\"}, \"text\": {\"verbosity\": \"medium\"}}\n",
    "    }\n",
    "    \n",
    "    config = settings.get(optimize_for, settings[\"balanced\"])\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = task\n",
    "    if code_context:\n",
    "        prompt = f\"Task: {task}\\n\\nExisting code:\\n```python\\n{code_context}\\n```\"\n",
    "    \n",
    "    # Call GPT-5\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=prompt,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "# Example usage\n",
    "result = code_assistant(\n",
    "    task=\"Add error handling and logging to this function\",\n",
    "    code_context=\"\"\"def divide(a, b):\n",
    "    return a / b\"\"\",\n",
    "    optimize_for=\"quality\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "GPT-5 represents a significant leap in AI reasoning capabilities. Key takeaways:\n",
    "\n",
    "1. **Use the Responses API** for multi-turn conversations to leverage CoT persistence\n",
    "2. **Configure reasoning and verbosity** based on your latency/quality requirements  \n",
    "3. **Leverage new features** like custom tools and allowed tools for better control\n",
    "4. **Choose the right model variant** (gpt-5, gpt-5-mini, gpt-5-nano) for your use case\n",
    "5. **Migrate gradually** using the recommended settings for your current model\n",
    "\n",
    "For more information:\n",
    "- [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/)\n",
    "- [GPT-5 Prompting Guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)\n",
    "- [GPT-5 Frontend Development](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)\n",
    "- [API Documentation](https://platform.openai.com/docs/guides/latest-model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
