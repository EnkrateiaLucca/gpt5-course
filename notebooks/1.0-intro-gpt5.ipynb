{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Using GPT-5 - OpenAI API Guide\n",
    "\n",
    "## Introduction to OpenAI's Most Intelligent Model\n",
    "\n",
    "GPT-5 is OpenAI's most advanced reasoning model, specifically trained for:\n",
    "- **Code generation**, bug fixing, and refactoring\n",
    "- **Instruction following** with high accuracy\n",
    "- **Long context** handling and **tool calling** for agentic tasks\n",
    "\n",
    "This notebook demonstrates GPT-5's key features with practical code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-cell",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the OpenAI Python client if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI client\n",
    "# Run this if you're running this notebook on google colab\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")  # Set your API key as environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-overview",
   "metadata": {},
   "source": [
    "## GPT-5 Model Variants\n",
    "\n",
    "| Model | Best For | Trade-offs |\n",
    "|-------|----------|------------|\n",
    "| **`gpt-5`** | Complex reasoning, broad knowledge, code-heavy tasks | Highest capability, higher latency |\n",
    "| **`gpt-5-mini`** | Cost-optimized reasoning and chat | Balanced speed, cost, and capability |\n",
    "| **`gpt-5-nano`** | High-throughput, simple tasks | Fastest, most cost-effective |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123b33e",
   "metadata": {},
   "source": [
    "System card uses different names than the API, OpenAI provides this mapping table:\n",
    "\n",
    "<img src=\"../assets/map-api-system-names.png\" width=40%>\n",
    "\n",
    "source: https://platform.openai.com/docs/guides/latest-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart-title",
   "metadata": {},
   "source": [
    "## Quickstart: Fast Responses with Low Reasoning\n",
    "\n",
    "For faster, lower-latency responses similar to GPT-4.1, use **low reasoning effort** and **low verbosity**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "quickstart-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: In lines of code, dawn\n",
      "Logic hums beneath the keys\n",
      "Bugs become insight\n",
      "\n",
      "Reasoning tokens used: N/A\n"
     ]
    }
   ],
   "source": [
    "# Fast response with minimal reasoning\n",
    "result = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a haiku about code.\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    ")\n",
    "\n",
    "print(\"Output:\", result.output_text)\n",
    "print(\"\\nReasoning tokens used:\", len(result.reasoning_text.split()) if hasattr(result, 'reasoning_text') else 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasoning-levels",
   "metadata": {},
   "source": [
    "## Reasoning Effort Control\n",
    "\n",
    "GPT-5 supports four reasoning levels: `minimal` (new one!) `low`, `medium`, `high`\n",
    "\n",
    "- **`minimal`**: Fastest time-to-first-token, best for coding & instruction following\n",
    "- **`low`**: Quick responses with light reasoning\n",
    "- **`medium`**: Default, balanced reasoning (similar to o3)\n",
    "- **`high`**: Most thorough reasoning for complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "minimal-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal Reasoning Output:\n",
      "Here’s a concise and efficient Python function to check if a number is prime:\n",
      "\n",
      "```python\n",
      "def is_prime(n: int) -> bool:\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "```\n",
      "\n",
      "Notes:\n",
      "- Handles small cases explicitly.\n",
      "- Eliminates even numbers and multiples of 3 early.\n",
      "- Checks divisibility up to sqrt(n) using 6k ± 1 optimization for speed.\n"
     ]
    }
   ],
   "source": [
    "# Minimal reasoning for fastest response\n",
    "response_minimal = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a Python function to check if a number is prime.\",\n",
    "    reasoning={\"effort\": \"minimal\"}\n",
    ")\n",
    "\n",
    "print(\"Minimal Reasoning Output:\")\n",
    "print(response_minimal.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "high-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Reasoning Output:\n",
      "Short answer: about 25–30 metric tons of gold. A reasonable estimate using the statue’s known copper skin gives about 25.5 metric tons.\n",
      "\n",
      "Assumptions and data\n",
      "- Coating only the statue (not the pedestal), exterior surface only\n",
      "- Copper skin thickness of the statue: 3/32 in = 0.09375 in = 0.00238125 m\n",
      "- Mass of the statue’s copper skin: 62,000 lb = 28,124 kg\n",
      "- Density of copper ρCu = 8,960 kg/m^3\n",
      "- Density of gold ρAu = 19,320 kg/m^3\n",
      "- Desired gold thickness tAu = 1 mm = 0.001 m\n",
      "\n",
      "Steps\n",
      "1) Infer the statue’s exterior surface area A from the copper skin:\n",
      "   A = mCu / (ρCu × tCu)\n",
      "     = 28,124 kg / (8,960 kg/m^3 × 0.00238125 m)\n",
      "     ≈ 28,124 / 21.336\n",
      "     ≈ 1,318 m^2\n",
      "\n",
      "2) Volume of gold needed for a 1 mm coat:\n",
      "   VAu = A × tAu = 1,318 m^2 × 0.001 m = 1.318 m^3\n",
      "\n",
      "3) Mass of gold:\n",
      "   mAu = ρAu × VAu = 19,320 kg/m^3 × 1.318 m^3 ≈ 25,458 kg\n",
      "\n",
      "Result\n",
      "- Gold required ≈ 25,500 kg ≈ 25.5 metric tons (≈ 28.1 short tons)\n",
      "\n",
      "Note\n",
      "- If you prefer to use a different surface-area estimate, the quick rule is:\n",
      "  mAu (kg) ≈ 19.32 × A (m^2) for a 1 mm coat.\n",
      "  For example, if A were 1,800 m^2, you’d need ≈ 34.8 metric tons.\n"
     ]
    }
   ],
   "source": [
    "# High reasoning for complex problems\n",
    "response_high = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"How much gold would it take to coat the Statue of Liberty in a 1mm layer? Show your calculations.\",\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "\n",
    "print(\"High Reasoning Output:\")\n",
    "print(response_high.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbosity-title",
   "metadata": {},
   "source": [
    "## Verbosity Control\n",
    "\n",
    "Control output length with `verbosity` parameter:\n",
    "- **`low`**: Concise answers, minimal code comments\n",
    "- **`medium`**: Balanced explanations (default)\n",
    "- **`high`**: Thorough explanations, detailed code documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "verbosity-low",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise Output:\n",
      "SELECT c.id AS customer_id,\n",
      "       c.name AS customer_name,\n",
      "       SUM(o.total_amount) AS total_purchase_amount\n",
      "FROM customers c\n",
      "JOIN orders o ON o.customer_id = c.id\n",
      "GROUP BY c.id, c.name\n",
      "ORDER BY total_purchase_amount DESC\n",
      "LIMIT 5;  -- For SQL Server use: SELECT TOP 5 ... (and remove LIMIT)\n"
     ]
    }
   ],
   "source": [
    "# Low verbosity for concise responses\n",
    "response_concise = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Generate a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "print(\"Concise Output:\")\n",
    "print(response_concise.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "verbosity-high",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Output:\n",
      "Async/await is JavaScript’s syntax for writing promise-based code that looks and reads like synchronous code. It doesn’t block the thread; it just lets you pause inside an async function until a promise settles, then continue with the resolved value or throw on rejection.\n",
      "\n",
      "Key ideas\n",
      "\n",
      "- async function\n",
      "  - Declaring a function with `async` makes it return a Promise.\n",
      "  - `return value` resolves the returned promise with `value`.\n",
      "  - `throw error` rejects the returned promise with `error`.\n",
      "\n",
      "- await expression\n",
      "  - You can only use `await` inside an `async` function (except for “top-level await” in ES modules).\n",
      "  - `await x` pauses the async function until `x` is settled. It is equivalent to `Promise.resolve(x)`:\n",
      "    - If `x` is a promise/thenable, it waits for it.\n",
      "    - If `x` is a plain value, it wraps it in a resolved promise and yields to the microtask queue once before continuing.\n",
      "  - On fulfillment, `await` gives you the value; on rejection, it throws an error you can catch with `try/catch`.\n",
      "\n",
      "- It is syntactic sugar over promises\n",
      "  - `const v = await p` is roughly `p.then(v => { /* use v */ }).catch(err => { throw err })`, but with cleaner control flow and error handling.\n",
      "\n",
      "Event loop and execution order (why it “pauses” without blocking)\n",
      "\n",
      "- JavaScript runs to completion; it never blocks the main thread on `await`.\n",
      "- When you hit `await`, the rest of the function is scheduled as a microtask to run after the current call stack finishes and after all currently queued microtasks for already-resolved promises.\n",
      "- Example ordering:\n",
      "  - Code:\n",
      "    - console.log('A')\n",
      "    - async function f() {\n",
      "        console.log('B')\n",
      "        await 0         // coerced to an already-resolved promise; yields to microtasks\n",
      "        console.log('C')\n",
      "      }\n",
      "    - f()\n",
      "    - console.log('D')\n",
      "  - Output: A, B, D, C\n",
      "\n",
      "Error handling patterns\n",
      "\n",
      "- Inside the async function, use `try/catch`:\n",
      "  - async function run() {\n",
      "      try {\n",
      "        const data = await fetch('/api').then(r => r.json())\n",
      "        return data\n",
      "      } catch (err) {\n",
      "        // handle or rethrow\n",
      "        throw err\n",
      "      }\n",
      "    }\n",
      "- At the call site, handle the returned promise:\n",
      "  - run().then(useData).catch(handleError)\n",
      "  - Or inside another async function: `try { await run() } catch (e) { ... }`\n",
      "\n",
      "- Nuance: `return await p` vs `return p`\n",
      "  - `return p` lets the async function’s returned promise adopt `p`’s state, but a surrounding `try/catch` will not catch `p`’s rejection inside the same function.\n",
      "  - `return await p` will throw inside the function on rejection, so a local `try/catch` can handle it.\n",
      "\n",
      "Sequential vs parallel\n",
      "\n",
      "- Sequential (slower if independent):\n",
      "  - const a = await fetch('/a')\n",
      "  - const b = await fetch('/b')\n",
      "- Parallel (start both, then await both):\n",
      "  - const [a, b] = await Promise.all([fetch('/a'), fetch('/b')])\n",
      "- Partial success:\n",
      "  - const results = await Promise.allSettled([p1, p2, p3])\n",
      "\n",
      "Loops and common pitfalls\n",
      "\n",
      "- Don’t use `Array.prototype.forEach` with `async` callbacks; it won’t await:\n",
      "  - Bad: `items.forEach(async item => { await doWork(item) })` // caller continues immediately\n",
      "- Use `for...of` for sequential behavior:\n",
      "  - for (const item of items) { await doWork(item) }\n",
      "- Use `map` + `Promise.all` for parallel:\n",
      "  - await Promise.all(items.map(item => doWork(item)))\n",
      "\n",
      "- Top-level await\n",
      "  - Available in ES modules (browsers and Node.js ESM). Blocks the module’s evaluation until the awaited promise settles, but does not block the overall event loop. Not available in classic scripts or CommonJS.\n",
      "\n",
      "Utilities and patterns\n",
      "\n",
      "- Sleep/Delay:\n",
      "  - const sleep = ms => new Promise(r => setTimeout(r, ms))\n",
      "  - await sleep(500)\n",
      "- Timeouts with race:\n",
      "  - const timeout = ms => new Promise((_, rej) => setTimeout(() => rej(new Error('Timeout')), ms))\n",
      "  - const res = await Promise.race([fetch(url), timeout(5000)])\n",
      "- Concurrency limiting (simple chunking):\n",
      "  - const chunks = []\n",
      "  - for (let i = 0; i < tasks.length; i += 5) chunks.push(tasks.slice(i, i + 5))\n",
      "  - for (const chunk of chunks) { await Promise.all(chunk.map(runTask)) }\n",
      "\n",
      "Interoperability and coercion\n",
      "\n",
      "- `await` works with any thenable (an object with a `.then` method), not just native promises.\n",
      "- `await`ing a non-promise value yields control once, then resumes with the value (handy trivia, affects ordering).\n",
      "\n",
      "Does async/await block the main thread?\n",
      "\n",
      "- No. The async function yields control to the event loop at each `await`. Other tasks (UI updates, I/O callbacks, timers) can run in the meantime.\n",
      "\n",
      "Under the hood\n",
      "\n",
      "- The JS engine transforms async functions into state machines that chain promises (historically via generators plus a runtime in transpiled output). It’s syntactic sugar that improves readability and error handling without changing the underlying promise semantics.\n",
      "\n",
      "Small end-to-end example\n",
      "\n",
      "- async function getUser(id) {\n",
      "    const res = await fetch(`/api/users/${id}`)\n",
      "    if (!res.ok) throw new Error('HTTP ' + res.status)\n",
      "    return res.json()\n",
      "  }\n",
      "- async function main() {\n",
      "    try {\n",
      "      const [alice, bob] = await Promise.all([getUser(1), getUser(2)])\n",
      "      console.log(alice.name, bob.name)\n",
      "    } catch (err) {\n",
      "      console.error('Failed to load users:', err)\n",
      "    }\n",
      "  }\n",
      "- main()\n",
      "\n",
      "If you share a specific snippet or scenario, I can show how to convert promise chains into async/await, or how to make a loop run sequentially vs in parallel safely....\n"
     ]
    }
   ],
   "source": [
    "# High verbosity for detailed explanations\n",
    "# response_detailed = client.responses.create(\n",
    "#     model=\"gpt-5\",\n",
    "#     input=\"Explain how async/await works in JavaScript.\",\n",
    "#     text={\"verbosity\": \"high\"}\n",
    "# )\n",
    "\n",
    "print(\"Detailed Output:\")\n",
    "print(response_detailed.output_text + \"...\")  # Truncated for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9a6024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "5334\n"
     ]
    }
   ],
   "source": [
    "print(len(response_concise.output_text))\n",
    "\n",
    "print(len(response_detailed.output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5932fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of cost for verbosity high vs low is\n",
      "18.204778156996586\n"
     ]
    }
   ],
   "source": [
    "output_cost_verbosity_low = 293*10/1000000\n",
    "output_cost_verbosity_high = 5334*10/1000000\n",
    "\n",
    "print(\"Ratio of cost for verbosity high vs low is\")\n",
    "print(output_cost_verbosity_high/output_cost_verbosity_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731da33",
   "metadata": {},
   "source": [
    "- High verbosity: Use when you need the model to provide thorough explanations of documents or perform extensive code refactoring.\n",
    "\n",
    "- Low verbosity: Best for situations where you want concise answers or simple code generation, such as SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-tools-title",
   "metadata": {},
   "source": [
    "## Custom Tools: Freeform Text Inputs\n",
    "\n",
    "GPT-5 introduces **custom tools** that accept raw text instead of structured JSON. Perfect for:\n",
    "- Executing code snippets\n",
    "- SQL queries\n",
    "- Shell commands\n",
    "- Configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "custom-tool-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call Generated:\n",
      "Tool: code_exec\n",
      "Input: import math\n",
      "print(math.factorial(10))\n"
     ]
    }
   ],
   "source": [
    "# Define a custom tool for code execution\n",
    "response_with_tool = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Use the code_exec tool to calculate the factorial of 10.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"code_exec\",\n",
    "            \"description\": \"Executes arbitrary Python code that prints something and returns the result\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Tool Call Generated:\")\n",
    "for reasoning_output in response_with_tool.output:\n",
    "    if reasoning_output.type == \"custom_tool_call\":\n",
    "        print(f\"Tool: {reasoning_output.name}\")\n",
    "        print(f\"Input: {reasoning_output.input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810946c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3628800'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def code_exec(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes arbitrary Python code and returns the result as a string.\n",
    "    WARNING: This function is dangerous and should only be used in secure, sandboxed environments.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import io\n",
    "    import traceback\n",
    "\n",
    "    # Redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    result = \"\"\n",
    "    try:\n",
    "        # Try to compile as an expression first\n",
    "        try:\n",
    "            compiled = compile(code, \"<string>\", \"eval\")\n",
    "            output = eval(compiled, {}, {})\n",
    "            if output is not None:\n",
    "                print(output)\n",
    "        except SyntaxError:\n",
    "            # If not an expression, treat as statements\n",
    "            exec(code, {}, {})\n",
    "        result = sys.stdout.getvalue()\n",
    "    except Exception:\n",
    "        result = \"Error:\\n\" + traceback.format_exc()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return result.strip()\n",
    "\n",
    "code_exec(reasoning_output.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg-title",
   "metadata": {},
   "source": [
    "## Context-Free Grammar (CFG) Constraints\n",
    "\n",
    "Constrain custom tool outputs to specific syntax using Lark grammars (or regex grammrs). \n",
    "\n",
    "We'll see a detailed example in notebook 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allowed-tools-title",
   "metadata": {},
   "source": [
    "## Allowed Tools: Selective Tool Access\n",
    "\n",
    "Define a full toolkit but restrict which tools can be used in specific contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "allowed-tools-example",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_68d691511d5081978f965991433591860257326b2b51fa3d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{}', call_id='call_jmfCsbT4SMmwhJpplgvZojSl', name='get_weather', type='function_call', id='fc_68d69155d3148197a52b0a1e93bd5dfb0257326b2b51fa3d', status='completed'),\n",
       " ResponseFunctionToolCall(arguments='{}', call_id='call_PArxuLpuaC7kF6Bq01RPvkMr', name='search_docs', type='function_call', id='fc_68d69155e72081979fbeedce747ba4230257326b2b51fa3d', status='completed')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define multiple tools but restrict usage\n",
    "all_tools = [\n",
    "    {\"type\": \"function\", \"name\": \"get_weather\", \"description\": \"Get current weather\"},\n",
    "    {\"type\": \"function\", \"name\": \"search_docs\", \"description\": \"Search documentation\"},\n",
    "    {\"type\": \"function\", \"name\": \"run_tests\", \"description\": \"Execute test suite\"},\n",
    "    {\"type\": \"function\", \"name\": \"deploy_code\", \"description\": \"Deploy to production\"}\n",
    "]\n",
    "\n",
    "# Only allow safe operations\n",
    "response_restricted = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What's the weather like and can you search for React hooks documentation?\",\n",
    "    tools=all_tools,\n",
    "    tool_choice={\n",
    "        \"type\": \"allowed_tools\",\n",
    "        \"mode\": \"auto\",  # Model decides which to use\n",
    "        \"tools\": [\n",
    "            {\"type\": \"function\", \"name\": \"get_weather\"},\n",
    "            {\"type\": \"function\", \"name\": \"search_docs\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "response_restricted.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fc0c589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function', 'name': 'get_weather'},\n",
       " {'type': 'function', 'name': 'search_docs'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_restricted.tool_choice.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preambles-title",
   "metadata": {},
   "source": [
    "## Tool Preambles for Transparency\n",
    "\n",
    "Enable preambles to see GPT-5's reasoning before tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "preambles-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with preamble:\n",
      "I will use the search tool to fetch authoritative, up-to-date references and tutorials on Python decorators so I can provide accurate and concise information.\n"
     ]
    }
   ],
   "source": [
    "# Enable preambles for tool transparency\n",
    "response_preamble = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Before you call a tool, explain why you are calling it. Now search for information about Python decorators.\",\n",
    "    tools=[\n",
    "        {\"type\": \"function\", \"name\": \"search_docs\", \"description\": \"Search documentation\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response with preamble:\")\n",
    "print(response_preamble.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-title",
   "metadata": {},
   "source": [
    "## Migration Guide\n",
    "\n",
    "### From Other Models to GPT-5\n",
    "\n",
    "| From Model | Migrate To | Recommended Settings |\n",
    "|------------|------------|---------------------|\n",
    "| **o3** | `gpt-5` | `reasoning.effort: \"medium\"` or `\"high\"` |\n",
    "| **gpt-4.1** | `gpt-5` | `reasoning.effort: \"minimal\"` or `\"low\"` |\n",
    "| **o4-mini** | `gpt-5-mini` | Default settings with prompt tuning |\n",
    "| **gpt-4.1-nano** | `gpt-5-nano` | Default settings with prompt tuning |\n",
    "\n",
    "### ⚠️ Important: Unsupported Parameters\n",
    "\n",
    "GPT-5 does **NOT** support:\n",
    "- `temperature`\n",
    "- `top_p` \n",
    "- `logprobs`\n",
    "\n",
    "Use GPT-5-specific controls instead:\n",
    "- `reasoning: {effort: ...}`\n",
    "- `text: {verbosity: ...}`\n",
    "- `max_output_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responses-api-title",
   "metadata": {},
   "source": [
    "## Responses API vs Chat Completions\n",
    "\n",
    "### Key Advantage: Chain of Thought (CoT) Persistence\n",
    "\n",
    "The Responses API passes reasoning between turns, resulting in:\n",
    "- Improved intelligence\n",
    "- Fewer reasoning tokens generated\n",
    "- Higher cache hit rates\n",
    "- Lower latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "multi-turn-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: “Optimal” depends on what you need:\n",
      "\n",
      "- Fastest and simplest for function memoization: use functools.lru_cache (implemented in C).\n",
      "- General-purpose key/value LRU cache with O(1) get/put: use OrderedDi...\n",
      "\n",
      "Follow-up (with CoT context): Here’s a thread-safe LRU cache (O(1) get/put) built on OrderedDict with an internal RLock. It snapshots iteration results to avoid races during traversal.\n",
      "\n",
      "from collections import OrderedDict\n",
      "import t...\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with CoT persistence\n",
    "first_response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Let's solve a complex problem. What's the optimal way to implement a LRU cache in Python?\",\n",
    "    reasoning={\"effort\": \"medium\"}\n",
    ")\n",
    "\n",
    "print(\"First response:\", first_response.output_text[:200] + \"...\")\n",
    "\n",
    "# Continue conversation, passing previous response ID\n",
    "follow_up = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Now add thread-safety to that implementation.\",\n",
    "    previous_response_id=first_response.id  # Passes CoT from previous turn\n",
    ")\n",
    "\n",
    "print(\"\\nFollow-up (with CoT context):\", follow_up.output_text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-title",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Choose the Right Model\n",
    "- **`gpt-5`**: Complex reasoning, coding, multi-step tasks\n",
    "- **`gpt-5-mini`**: General chat, moderate complexity\n",
    "- **`gpt-5-nano`**: Simple tasks, high throughput\n",
    "\n",
    "### 2. Optimize for Your Use Case\n",
    "- **Speed Priority**: Use `minimal` reasoning + `low` verbosity\n",
    "- **Quality Priority**: Use `high` reasoning + `high` verbosity\n",
    "- **Balanced**: Use defaults (`medium` for both)\n",
    "\n",
    "### 3. Leverage New Features\n",
    "- **Custom Tools**: For code execution, SQL, configs\n",
    "- **Allowed Tools**: For safety and predictability\n",
    "- **Preambles**: For debugging and transparency\n",
    "\n",
    "### 4. Use Responses API for Multi-turn\n",
    "- Always pass `previous_response_id` for context\n",
    "- Reduces re-reasoning and improves coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-example-title",
   "metadata": {},
   "source": [
    "## Practical Example: Building a Code Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "code-assistant-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a version with error handling and logging:\n",
      "\n",
      "```python\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def divide(a, b):\n",
      "    \"\"\"\n",
      "    Divide a by b with logging and error handling.\n",
      "\n",
      "    - Logs inputs and result at DEBUG level.\n",
      "    - Logs and re-raises exceptions for invalid inputs or division by zero.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        logger.debug(\"divide called with a=%r, b=%r\", a, b)\n",
      "        result = a / b\n",
      "    except ZeroDivisionError:\n",
      "        logger.exception(\"Division by zero: a=%r, b=%r\", a, b)\n",
      "        raise\n",
      "    except TypeError:\n",
      "        logger.exception(\n",
      "            \"Unsupported operand types for division: a=%r (%s), b=%r (%s)\",\n",
      "            a, type(a).__name__, b, type(b).__name__\n",
      "        )\n",
      "        raise\n",
      "    except Exception:\n",
      "        logger.exception(\"Unexpected error during division: a=%r, b=%r\", a, b)\n",
      "        raise\n",
      "    else:\n",
      "        logger.debug(\"division result: %r\", result)\n",
      "        return result\n",
      "```\n",
      "\n",
      "Note:\n",
      "- In application code, configure logging once (avoid doing this in libraries/modules):\n",
      "  ```python\n",
      "  import logging\n",
      "  logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s:%(name)s:%(message)s\")\n",
      "  ```\n"
     ]
    }
   ],
   "source": [
    "def code_assistant(task, code_context=None, optimize_for=\"balanced\"):\n",
    "    \"\"\"\n",
    "    GPT-5 powered code assistant with configurable optimization.\n",
    "    \n",
    "    Args:\n",
    "        task: What you want the assistant to do\n",
    "        code_context: Existing code to work with\n",
    "        optimize_for: \"speed\", \"quality\", or \"balanced\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure based on optimization preference\n",
    "    settings = {\n",
    "        \"speed\": {\"reasoning\": {\"effort\": \"minimal\"}, \"text\": {\"verbosity\": \"low\"}},\n",
    "        \"quality\": {\"reasoning\": {\"effort\": \"high\"}, \"text\": {\"verbosity\": \"high\"}},\n",
    "        \"balanced\": {\"reasoning\": {\"effort\": \"medium\"}, \"text\": {\"verbosity\": \"medium\"}}\n",
    "    }\n",
    "    \n",
    "    config = settings.get(optimize_for, settings[\"balanced\"])\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = task\n",
    "    if code_context:\n",
    "        prompt = f\"Task: {task}\\n\\nExisting code:\\n```python\\n{code_context}\\n```\"\n",
    "    \n",
    "    # Call GPT-5\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=prompt,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "# Example usage\n",
    "result = code_assistant(\n",
    "    task=\"Add error handling and logging to this function\",\n",
    "    code_context=\"\"\"def divide(a, b):\n",
    "    return a / b\"\"\",\n",
    "    optimize_for=\"quality\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "GPT-5 represents a significant leap in AI reasoning capabilities. Key takeaways:\n",
    "\n",
    "1. **Use the Responses API** for multi-turn conversations to leverage CoT persistence\n",
    "2. **Configure reasoning and verbosity** based on your latency/quality requirements  \n",
    "3. **Leverage new features** like custom tools and allowed tools for better control\n",
    "4. **Choose the right model variant** (gpt-5, gpt-5-mini, gpt-5-nano) for your use case\n",
    "5. **Migrate gradually** using the recommended settings for your current model\n",
    "\n",
    "For more information:\n",
    "- [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/)\n",
    "- [GPT-5 Prompting Guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)\n",
    "- [GPT-5 Frontend Development](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)\n",
    "- [API Documentation](https://platform.openai.com/docs/guides/latest-model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
