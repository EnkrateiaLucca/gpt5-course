{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Using GPT-5 - OpenAI API Guide\n",
    "\n",
    "## Introduction to OpenAI's Most Intelligent Model\n",
    "\n",
    "GPT-5 is OpenAI's most advanced reasoning model, specifically trained for:\n",
    "- **Code generation**, bug fixing, and refactoring\n",
    "- **Instruction following** with high accuracy\n",
    "- **Long context** handling and **tool calling** for agentic tasks\n",
    "\n",
    "This notebook demonstrates GPT-5's key features with practical code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-cell",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the OpenAI Python client if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI client\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")  # Set your API key as environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-overview",
   "metadata": {},
   "source": [
    "## GPT-5 Model Variants\n",
    "\n",
    "| Model | Best For | Trade-offs |\n",
    "|-------|----------|------------|\n",
    "| **`gpt-5`** | Complex reasoning, broad knowledge, code-heavy tasks | Highest capability, higher latency |\n",
    "| **`gpt-5-mini`** | Cost-optimized reasoning and chat | Balanced speed, cost, and capability |\n",
    "| **`gpt-5-nano`** | High-throughput, simple tasks | Fastest, most cost-effective |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quickstart-title",
   "metadata": {},
   "source": [
    "## Quickstart: Fast Responses with Low Reasoning\n",
    "\n",
    "For faster, lower-latency responses similar to GPT-4.1, use **low reasoning effort** and **low verbosity**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickstart-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast response with minimal reasoning\n",
    "result = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a haiku about code.\",\n",
    "    reasoning={\"effort\": \"low\"},\n",
    "    text={\"verbosity\": \"low\"},\n",
    ")\n",
    "\n",
    "print(\"Output:\", result.output_text)\n",
    "print(\"\\nReasoning tokens used:\", len(result.reasoning_text.split()) if hasattr(result, 'reasoning_text') else 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasoning-levels",
   "metadata": {},
   "source": [
    "## Reasoning Effort Control\n",
    "\n",
    "GPT-5 supports four reasoning levels: `minimal`, `low`, `medium`, `high`\n",
    "\n",
    "- **`minimal`**: Fastest time-to-first-token, best for coding & instruction following\n",
    "- **`low`**: Quick responses with light reasoning\n",
    "- **`medium`**: Default, balanced reasoning (similar to o3)\n",
    "- **`high`**: Most thorough reasoning for complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal reasoning for fastest response\n",
    "response_minimal = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a Python function to check if a number is prime.\",\n",
    "    reasoning={\"effort\": \"minimal\"}\n",
    ")\n",
    "\n",
    "print(\"Minimal Reasoning Output:\")\n",
    "print(response_minimal.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "high-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High reasoning for complex problems\n",
    "response_high = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"How much gold would it take to coat the Statue of Liberty in a 1mm layer? Show your calculations.\",\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "\n",
    "print(\"High Reasoning Output:\")\n",
    "print(response_high.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbosity-title",
   "metadata": {},
   "source": [
    "## Verbosity Control\n",
    "\n",
    "Control output length with `verbosity` parameter:\n",
    "- **`low`**: Concise answers, minimal code comments\n",
    "- **`medium`**: Balanced explanations (default)\n",
    "- **`high`**: Thorough explanations, detailed code documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbosity-low",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low verbosity for concise responses\n",
    "response_concise = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Generate a SQL query to find the top 5 customers by total purchase amount.\",\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "print(\"Concise Output:\")\n",
    "print(response_concise.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbosity-high",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High verbosity for detailed explanations\n",
    "response_detailed = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Explain how async/await works in JavaScript.\",\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "print(\"Detailed Output:\")\n",
    "print(response_detailed.output_text[:500] + \"...\")  # Truncated for display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-tools-title",
   "metadata": {},
   "source": [
    "## Custom Tools: Freeform Text Inputs\n",
    "\n",
    "GPT-5 introduces **custom tools** that accept raw text instead of structured JSON. Perfect for:\n",
    "- Executing code snippets\n",
    "- SQL queries\n",
    "- Shell commands\n",
    "- Configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-tool-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom tool for code execution\n",
    "response_with_tool = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Use the code_exec tool to calculate the factorial of 10.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"code_exec\",\n",
    "            \"description\": \"Executes arbitrary Python code and returns the result\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Tool Call Generated:\")\n",
    "if hasattr(response_with_tool, 'tool_calls'):\n",
    "    for tool_call in response_with_tool.tool_calls:\n",
    "        print(f\"Tool: {tool_call.name}\")\n",
    "        print(f\"Input: {tool_call.input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg-title",
   "metadata": {},
   "source": [
    "## Context-Free Grammar (CFG) Constraints\n",
    "\n",
    "Constrain custom tool outputs to specific syntax using Lark grammars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SQL query tool with grammar constraints\n",
    "sql_grammar = \"\"\"\n",
    "start: select_stmt\n",
    "select_stmt: \"SELECT\" columns \"FROM\" table [where_clause] [order_clause] [limit_clause]\n",
    "columns: \"*\" | column (\",\" column)*\n",
    "column: WORD\n",
    "table: WORD\n",
    "where_clause: \"WHERE\" condition\n",
    "condition: column operator value\n",
    "operator: \"=\" | \">\" | \"<\" | \">=\" | \"<=\" | \"LIKE\"\n",
    "value: STRING | NUMBER\n",
    "order_clause: \"ORDER BY\" column [\"ASC\" | \"DESC\"]\n",
    "limit_clause: \"LIMIT\" NUMBER\n",
    "\n",
    "%import common.WORD\n",
    "%import common.STRING\n",
    "%import common.NUMBER\n",
    "\"\"\"\n",
    "\n",
    "response_sql = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Create a SQL query to find users named John ordered by registration date.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"sql_query\",\n",
    "            \"description\": \"Generates SQL queries\",\n",
    "            \"grammar\": sql_grammar\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Constrained SQL Output:\")\n",
    "if hasattr(response_sql, 'tool_calls'):\n",
    "    print(response_sql.tool_calls[0].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allowed-tools-title",
   "metadata": {},
   "source": [
    "## Allowed Tools: Selective Tool Access\n",
    "\n",
    "Define a full toolkit but restrict which tools can be used in specific contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allowed-tools-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple tools but restrict usage\n",
    "all_tools = [\n",
    "    {\"type\": \"function\", \"name\": \"get_weather\", \"description\": \"Get current weather\"},\n",
    "    {\"type\": \"function\", \"name\": \"search_docs\", \"description\": \"Search documentation\"},\n",
    "    {\"type\": \"function\", \"name\": \"run_tests\", \"description\": \"Execute test suite\"},\n",
    "    {\"type\": \"function\", \"name\": \"deploy_code\", \"description\": \"Deploy to production\"}\n",
    "]\n",
    "\n",
    "# Only allow safe operations\n",
    "response_restricted = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What's the weather like and can you search for React hooks documentation?\",\n",
    "    tools=all_tools,\n",
    "    tool_choice={\n",
    "        \"type\": \"allowed_tools\",\n",
    "        \"mode\": \"auto\",  # Model decides which to use\n",
    "        \"tools\": [\n",
    "            {\"type\": \"function\", \"name\": \"get_weather\"},\n",
    "            {\"type\": \"function\", \"name\": \"search_docs\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Model can only access: get_weather, search_docs\")\n",
    "print(\"Model cannot access: run_tests, deploy_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preambles-title",
   "metadata": {},
   "source": [
    "## Tool Preambles for Transparency\n",
    "\n",
    "Enable preambles to see GPT-5's reasoning before tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preambles-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable preambles for tool transparency\n",
    "response_preamble = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Before you call a tool, explain why you are calling it. Now search for information about Python decorators.\",\n",
    "    tools=[\n",
    "        {\"type\": \"function\", \"name\": \"search_docs\", \"description\": \"Search documentation\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response with preamble:\")\n",
    "print(response_preamble.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-title",
   "metadata": {},
   "source": [
    "## Migration Guide\n",
    "\n",
    "### From Other Models to GPT-5\n",
    "\n",
    "| From Model | Migrate To | Recommended Settings |\n",
    "|------------|------------|---------------------|\n",
    "| **o3** | `gpt-5` | `reasoning.effort: \"medium\"` or `\"high\"` |\n",
    "| **gpt-4.1** | `gpt-5` | `reasoning.effort: \"minimal\"` or `\"low\"` |\n",
    "| **o4-mini** | `gpt-5-mini` | Default settings with prompt tuning |\n",
    "| **gpt-4.1-nano** | `gpt-5-nano` | Default settings with prompt tuning |\n",
    "\n",
    "### ⚠️ Important: Unsupported Parameters\n",
    "\n",
    "GPT-5 does **NOT** support:\n",
    "- `temperature`\n",
    "- `top_p` \n",
    "- `logprobs`\n",
    "\n",
    "Use GPT-5-specific controls instead:\n",
    "- `reasoning: {effort: ...}`\n",
    "- `text: {verbosity: ...}`\n",
    "- `max_output_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responses-api-title",
   "metadata": {},
   "source": [
    "## Responses API vs Chat Completions\n",
    "\n",
    "### Key Advantage: Chain of Thought (CoT) Persistence\n",
    "\n",
    "The Responses API passes reasoning between turns, resulting in:\n",
    "- Improved intelligence\n",
    "- Fewer reasoning tokens generated\n",
    "- Higher cache hit rates\n",
    "- Lower latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-turn-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation with CoT persistence\n",
    "first_response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Let's solve a complex problem. What's the optimal way to implement a LRU cache in Python?\",\n",
    "    reasoning={\"effort\": \"medium\"}\n",
    ")\n",
    "\n",
    "print(\"First response:\", first_response.output_text[:200] + \"...\")\n",
    "\n",
    "# Continue conversation, passing previous response ID\n",
    "follow_up = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Now add thread-safety to that implementation.\",\n",
    "    previous_response_id=first_response.id  # Passes CoT from previous turn\n",
    ")\n",
    "\n",
    "print(\"\\nFollow-up (with CoT context):\", follow_up.output_text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-title",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Choose the Right Model\n",
    "- **`gpt-5`**: Complex reasoning, coding, multi-step tasks\n",
    "- **`gpt-5-mini`**: General chat, moderate complexity\n",
    "- **`gpt-5-nano`**: Simple tasks, high throughput\n",
    "\n",
    "### 2. Optimize for Your Use Case\n",
    "- **Speed Priority**: Use `minimal` reasoning + `low` verbosity\n",
    "- **Quality Priority**: Use `high` reasoning + `high` verbosity\n",
    "- **Balanced**: Use defaults (`medium` for both)\n",
    "\n",
    "### 3. Leverage New Features\n",
    "- **Custom Tools**: For code execution, SQL, configs\n",
    "- **Allowed Tools**: For safety and predictability\n",
    "- **Preambles**: For debugging and transparency\n",
    "\n",
    "### 4. Use Responses API for Multi-turn\n",
    "- Always pass `previous_response_id` for context\n",
    "- Reduces re-reasoning and improves coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-example-title",
   "metadata": {},
   "source": [
    "## Practical Example: Building a Code Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-assistant-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_assistant(task, code_context=None, optimize_for=\"balanced\"):\n",
    "    \"\"\"\n",
    "    GPT-5 powered code assistant with configurable optimization.\n",
    "    \n",
    "    Args:\n",
    "        task: What you want the assistant to do\n",
    "        code_context: Existing code to work with\n",
    "        optimize_for: \"speed\", \"quality\", or \"balanced\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure based on optimization preference\n",
    "    settings = {\n",
    "        \"speed\": {\"reasoning\": {\"effort\": \"minimal\"}, \"text\": {\"verbosity\": \"low\"}},\n",
    "        \"quality\": {\"reasoning\": {\"effort\": \"high\"}, \"text\": {\"verbosity\": \"high\"}},\n",
    "        \"balanced\": {\"reasoning\": {\"effort\": \"medium\"}, \"text\": {\"verbosity\": \"medium\"}}\n",
    "    }\n",
    "    \n",
    "    config = settings.get(optimize_for, settings[\"balanced\"])\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt = task\n",
    "    if code_context:\n",
    "        prompt = f\"Task: {task}\\n\\nExisting code:\\n```python\\n{code_context}\\n```\"\n",
    "    \n",
    "    # Call GPT-5\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=prompt,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "# Example usage\n",
    "result = code_assistant(\n",
    "    task=\"Add error handling and logging to this function\",\n",
    "    code_context=\"\"\"def divide(a, b):\n",
    "    return a / b\"\"\",\n",
    "    optimize_for=\"quality\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "GPT-5 represents a significant leap in AI reasoning capabilities. Key takeaways:\n",
    "\n",
    "1. **Use the Responses API** for multi-turn conversations to leverage CoT persistence\n",
    "2. **Configure reasoning and verbosity** based on your latency/quality requirements  \n",
    "3. **Leverage new features** like custom tools and allowed tools for better control\n",
    "4. **Choose the right model variant** (gpt-5, gpt-5-mini, gpt-5-nano) for your use case\n",
    "5. **Migrate gradually** using the recommended settings for your current model\n",
    "\n",
    "For more information:\n",
    "- [GPT-5 System Card](https://openai.com/index/gpt-5-system-card/)\n",
    "- [GPT-5 Prompting Guide](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide)\n",
    "- [GPT-5 Frontend Development](https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend)\n",
    "- [API Documentation](https://platform.openai.com/docs/guides/latest-model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}